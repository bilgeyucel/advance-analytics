{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Data Pre-Processing & basic feature engineering","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPhploJ5DrRTfknzMf31iYf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"8yNIPDsaoPyJ"},"outputs":[],"source":["\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","import pickle\n","import datetime as dt\n","\n","#Useful packages for building deep neural networks. \n","import tensorflow as tf \n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv1D,Flatten,Dense,Dropout, Reshape,MaxPooling2D,Conv2D\n","from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n","\n","#Additional library which we will use for preprocessing our image data before training our model and to provide some specific evaluation metrics.\n","import sklearn\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import classification_report\n","from sklearn.preprocessing import RobustScaler"]},{"cell_type":"code","source":["#--- Helper Functions ---\n","def get_age_in_months(d1, d2):\n","    d1 = dt.datetime.strptime(d1, \"%Y-%m\")\n","    d2 = dt.datetime.strptime(d2, \"%Y-%m\")\n","    return int(abs((d2 - d1).days/30))\n","\n","def printMissingData(df):\n","  features = df.columns.values.tolist()\n","\n","  print(\"Number of rows:\",len(df))\n","  print(\"Missing data\")\n","  print(\"------------\")\n","  for feature in features:\n","      missing = (df[feature].isnull()).sum()\n","      if missing != 0:\n","          print(feature,':',missing,'->',(missing/len(df))*100,'%')\n","\n","def getRegionClass(postalCodes):\n","    Brussels = []\n","    Flanders = []\n","    Wallonia = []\n","    Other = []\n","    for code in postalCodes:\n","        #Brussels\n","        if ((code >= 1000) and (code <= 1212)) or ((code >= 1931) and (code <= 1950)):\n","            Brussels.append(1)\n","            Flanders.append(0)\n","            Wallonia.append(0)\n","            Other.append(0)\n","        #Flanders\n","        elif ((code >= 1500) and (code <= 4690)) or ((code >= 8000) and (code <= 9999)):\n","            Brussels.append(0)\n","            Flanders.append(1)\n","            Wallonia.append(0)\n","            Other.append(0)\n","        #Wallonia\n","        elif (code >= 4000) and (code <= 7970):\n","            Brussels.append(0)\n","            Flanders.append(0)\n","            Wallonia.append(1)\n","            Other.append(0)\n","        #Other\n","        else:\n","            Brussels.append(0)\n","            Flanders.append(0)\n","            Wallonia.append(0)\n","            Other.append(1)\n","    return Brussels,Flanders,Wallonia,Other\n","\n","def normalize_minmax(column):\n","    a,b = 0,1\n","    min,max = column.min(),column.max()\n","    return (column-min) / (max - min)\n","  \n","def normalize_robust(column):\n","    trans = RobustScaler()\n","    print('\\nmax before transform: {}'.format(max(column)))\n","    column = pd.DataFrame(column) #make 2D for robustscaler input\n","    column = trans.fit_transform(column)\n","    print(column.shape)\n","    column = pd.DataFrame(column).squeeze() #make 2D again\n","    print('\\nmax after transform: {}'.format(max(column)))\n","    return column\n","\n","\n","\n","\n","\n","def clean_minmax(data,transform=\"normal\",nan_handler=\"drop\"): #Nan handler can be set to \"drop\" or \"to_zero\", transform to \"normal\" , \"robust\" or \"power\"\n","  #------------------ STEP 1.1: HANDLE THE NAN VALUES -----------------------\n","\n","  if(nan_handler == \"drop\"):\n","    #Education seems to be MNAR! Much less churners have reported their education...\n","    printMissingData(data)\n","    #create feature indicating if education was reported (can be informative)\n","    data['has_reported_education'] = data['customer_education'].notnull().astype('int')\n","    #drop the columns with more than 25% missing values\n","    data.drop(['customer_education','customer_children','customer_relationship'],axis=1,inplace=True)\n","    #Remove all other rows containing NaN values (up to 4% loss in data)\n","    data_len = data.shape[0]\n","    data = data.dropna()\n","    print('\\nData loss after dropping NaNs: {}%'.format((data.shape[0]-data_len)/data_len*100))\n","\n","  elif(nan_handler == \"to_zero\"):\n","    printMissingData(data)\n","    #create feature indicating if education was reported (can be informative)\n","    data['has_reported_education'] = data['customer_education'].notnull().astype('int')\n","    #drop the columns with more than 25% missing values\n","    data.drop(['customer_education','customer_children','customer_relationship'],axis=1,inplace=True)\n","    #Fill nan data values with dataset's creation date\n","    data['customer_since_all'] = data['customer_since_all'].fillna(dt.datetime(2018, 6,1).strftime(\"%Y-%m\")) #CAN BE PROBLEM IF YOUNGER PEOPLE MORE LIKELY TO CHURN!\n","    data['customer_since_bank'] = data['customer_since_bank'].fillna(dt.datetime(2018, 6,1).strftime(\"%Y-%m\"))\n","    #Fill remaining nan values with 0s\n","    data.fillna(0)\n","\n","\n","  #------------------ STEP 1.2: HANDLE DATE TYPE FEATURES -----------------------\n","  dataset_creation_date = dt.datetime(2018, 6,1).strftime(\"%Y-%m\")\n","  # today = dt.date.today().strftime(\"%Y-%m\")\n","\n","\n","  #Turn dates into whole numbers (months passed since today)\n","  data[\"customer_since_all\"] = [get_age_in_months(x,dataset_creation_date) for x in data[\"customer_since_all\"]]\n","  data[\"customer_since_bank\"] = [get_age_in_months(x,dataset_creation_date) for x in data[\"customer_since_bank\"]]\n","  data[\"customer_birth_date\"] = [get_age_in_months(x,dataset_creation_date) for x in data[\"customer_birth_date\"]]\n","\n","\n","  #------------------ STEP 1.3: Transform the categorical variables -----------------------\n","  #Change POSTAL CODES to one-hot encoded regions\n","  Brussels,Flanders,Wallonia,Other = getRegionClass(data['customer_postal_code'])\n","  data['brussels_postal_code'] = Brussels\n","  data['flanders_postal_code'] = Flanders\n","  data['wallonia_postal_code'] = Wallonia\n","  data['other_postal_code'] = Other\n","  data.drop([\"customer_postal_code\"], axis=1,inplace=True)\n","\n","  #Change OCCUPATION CODES to one-hot encoded regions. DROP FOR NOW SINCE DISTRIBUTIONS BETWEEN CHURNERS AND CONTROL GROUP SEEM NEGLIGIBLE\n","  data.drop([\"customer_occupation_code\"], axis=1,inplace=True)\n","\n","\n","  #------------------ STEP 1.4: General operations -----------------------\n","  #turn visits into integers\n","  data[\"visits_distinct_so\"] = data[\"visits_distinct_so\"].astype(int)\n","  data[\"visits_distinct_so_areas\"] = data[\"visits_distinct_so_areas\"].astype(int)\n","\n","  #Turn gender into 0-1 instead of 1-2 encoding\n","  def gender_to_binary(x):\n","    if(x==1):\n","      return 0\n","    else:\n","      return 1\n","  data[\"customer_gender\"] = [gender_to_binary(x) for x in data[\"customer_gender\"]]\n","\n","\n","  #------------------ STEP 1.5: Normalization -----------------------\n","  cols_to_normalize = ['bal_insurance_21',\"bal_insurance_23\",\"cap_life_insurance_fixed_cap\",\"cap_life_insurance_decreasing_cap\",\"prem_fire_car_other_insurance\",\n","                      \"bal_personal_loan\",\"bal_mortgage_loan\",\"bal_current_account\",\"bal_pension_saving\",\"bal_savings_account\",\"bal_savings_account_starter\",\n","                      \"bal_current_account_starter\",\"visits_distinct_so\",\"visits_distinct_so_areas\",\"customer_since_all\",\"customer_since_bank\",\"customer_birth_date\"]\n","\n","  if(transform == \"normal\"):\n","    for x in cols_to_normalize:\n","      data[x] = normalize_minmax(data[x])\n","    return data\n","\n","  elif transform == \"robust\"):\n","    trans = RobustScaler()\n","    data[cols_to_normalize] = trans.fit_transform(data[cols_to_normalize])\n","    return data\n","\n","  elif(transform == \"power\"): #For yeo power transform\n","    from sklearn.preprocessing import power_transform\n","    data[cols_to_normalize] = power_transform(data[cols_to_normalize],method='yeo-johnson') #Can handle full dataframes\n","    return data\n","\n","  elif(transform==None):\n","    return data"],"metadata":{"id":"E1X9GfFVo8Hg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Define parent functions for data cleaning --- (Normally data is also saved to pass on to feature engineering and model notebooks)\n","\n","def clean_train_data(transform,nan_handler,filename):\n","  train_data1 = pd.read_csv('/content/drive/MyDrive/Studies/Master of AI/Advanced_Analytics/Assignment_1/data_original/train_month_1.csv')\n","  train_data1 = clean_robust(train_data1,transform=transform,nan_handler=nan_handler)\n","\n","  train_data2 = pd.read_csv('/content/drive/MyDrive/Studies/Master of AI/Advanced_Analytics/Assignment_1/data_original/train_month_2.csv')\n","  train_data2 = clean_robust(train_data2,transform=transform,nan_handler=nan_handler)\n","\n","  train_data3 = pd.read_csv('/content/drive/MyDrive/Studies/Master of AI/Advanced_Analytics/Assignment_1/data_original/train_month_3_with_target.csv')\n","  train_data3 = clean_robust(train_data3,transform=transform,nan_handler=nan_handler)\n","\n","  return train_data1, train_data2, train_data3\n","\n","\n","def clean_test_data(transform,nan_handler,filename):\n","  test_data1 = pd.read_csv('/content/drive/MyDrive/Studies/Master of AI/Advanced_Analytics/Assignment_1/data_original/test_month_1.csv')\n","  test_data1 = clean_minmax(test_data1,transform=transform,nan_handler=nan_handler)\n","\n","  test_data2 = pd.read_csv('/content/drive/MyDrive/Studies/Master of AI/Advanced_Analytics/Assignment_1/data_original/test_month_2.csv')\n","  test_data2 = clean_minmax(test_data2,transform=transform,nan_handler=nan_handler)\n","\n","  test_data3 = pd.read_csv('/content/drive/MyDrive/Studies/Master of AI/Advanced_Analytics/Assignment_1/data_original/test_month_3.csv')\n","  test_data3 = clean_minmax(test_data3,transform=transform,nan_handler=nan_handler)\n","\n","  return test_data1, test_data2, test_data3\n"],"metadata":{"id":"Z0VOOY-9o_dW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the cleaned training and testing data\n","train_data1, train_data2, train_data3 = clean_train_data(transform=\"robust\",nan_handler=\"drop\",filename=None)\n","test_data1, test_data2, test_data3 = clean_test_data(transform=\"robust\",nan_handler=\"drop\",filename=None)"],"metadata":{"id":"0Ared_EgqHlY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"UO78wfdAs4Kt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"RvPnFR4PuGbS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Manually engineer some features based on intuition and proven churn metrics (e.g. drop in account balance) ---"],"metadata":{"id":"3ujoqwdss4hK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_manual_features_df(data,transform,nan_handler,filename):\n","  #Get the original data\n","  if(data==\"train\"):\n","    df1 = pd.read_csv('/content/drive/MyDrive/Studies/Master of AI/Advanced_Analytics/Assignment_1/data_original/train_month_1.csv')\n","    df1 = clean_minmax(df1,transform=None,nan_handler=nan_handler)\n","    df1.sort_values(by=['client_id'],inplace=True) #Sort values for consistent row positions per customer across dataframes\n","    df3 = pd.read_csv('/content/drive/MyDrive/Studies/Master of AI/Advanced_Analytics/Assignment_1/data_original/train_month_3.csv')\n","    df3 = clean_minmax(df3,transform=None,nan_handler=nan_handler)\n","    df3.sort_values(by=['client_id'],inplace=True) #Sort values for consistent row positions per customer across dataframes\n","\n","  if(data=\"test\")\n","    df1 = pd.read_csv('/content/drive/MyDrive/Studies/Master of AI/Advanced_Analytics/Assignment_1/data_original/test_month_1.csv')\n","    df1 = clean_minmax(df1,transform=None,nan_handler=nan_handler)\n","    df1.sort_values(by=['client_id'],inplace=True) #Sort values for consistent row positions per customer across dataframes\n","    df3 = pd.read_csv('/content/drive/MyDrive/Studies/Master of AI/Advanced_Analytics/Assignment_1/data_original/test_month_3.csv')\n","    df3 = clean_minmax(df3,transform=None,nan_handler=nan_handler)\n","    df3.sort_values(by=['client_id'],inplace=True) #Sort values for consistent row positions per customer across dataframes\n","  \n","  \n","  #create df to hold transformed dataset \n","  new_df = pd.DataFrame({\"client_id\":df1['client_id']})\n","\n","  new_df[\"homebanking_change\"] = df3[\"has_homebanking\"] - df1[\"has_homebanking\"] #0 = nothing changed, 1 = activated it, -1 = deactivated it\n","  new_df[\"insurance_21_change\"] = df3[\"has_insurance_21\"] - df1[\"has_insurance_21\"]\n","  new_df[\"insurance_23_change\"] = df3[\"has_insurance_23\"] - df1[\"has_insurance_23\"]\n","  new_df[\"life_ins_fixed_cap_change\"] = df3[\"has_life_insurance_fixed_cap\"] - df1[\"has_life_insurance_fixed_cap\"]\n","  new_df[\"life_ins_decreas_cap_change\"] = df3[\"has_life_insurance_decreasing_cap\"] - df1[\"has_life_insurance_decreasing_cap\"]\n","  new_df[\"fire_insurance_change\"] = df3[\"has_fire_car_other_insurance\"] - df1[\"has_fire_car_other_insurance\"]\n","  new_df[\"personal_loan_change\"] = df3[\"has_personal_loan\"] - df1[\"has_personal_loan\"]\n","  new_df[\"mortgage_loan_change\"] = df3[\"has_mortgage_loan\"] - df1[\"has_mortgage_loan\"]\n","  new_df[\"current_acc_change\"] = df3[\"has_current_account\"] - df1[\"has_current_account\"]\n","  new_df[\"pension_savings_acc_change\"] = df3[\"has_pension_saving\"] - df1[\"has_pension_saving\"]\n","  new_df[\"savings_acc_change\"] = df3[\"has_savings_account\"] - df1[\"has_savings_account\"]\n","  new_df[\"savings_acc_starter_change\"] = df3[\"has_savings_account_starter\"] - df1[\"has_savings_account_starter\"]\n","  new_df[\"current_acc_starter_change\"] = df3[\"has_current_account_starter\"] - df1[\"has_current_account_starter\"]\n","  new_df[\"bal_change_insurance_21\"] = df3[\"bal_insurance_21\"] - df1[\"bal_insurance_21\"]\n","  new_df[\"bal_change_insurance_23\"] = df3[\"bal_insurance_23\"] - df1[\"bal_insurance_23\"]\n","  new_df[\"cap_life_ins_fixed\"] = df3[\"cap_life_insurance_fixed_cap\"] - df1[\"cap_life_insurance_fixed_cap\"]\n","  new_df[\"cap_life_ins_decreasing\"] = df3[\"cap_life_insurance_decreasing_cap\"] - df1[\"cap_life_insurance_decreasing_cap\"]\n","  new_df[\"fire_premium_change\"] = df3[\"prem_fire_car_other_insurance\"] - df1[\"prem_fire_car_other_insurance\"]\n","  new_df[\"bal_change_personal_loan\"] = df3[\"bal_personal_loan\"] - df1[\"bal_personal_loan\"]\n","  new_df[\"bal_change_mortgage_loan\"] = df3[\"bal_mortgage_loan\"] - df1[\"bal_mortgage_loan\"]\n","  new_df[\"bal_change_current_account\"] = df3[\"bal_current_account\"] - df1[\"bal_current_account\"]\n","  new_df[\"bal_change_pension_savings\"] = df3[\"bal_pension_saving\"] - df1[\"bal_pension_saving\"]\n","  new_df[\"bal_change_savings\"] = df3[\"bal_savings_account\"] - df1[\"bal_savings_account\"]\n","  new_df[\"bal_change_savings_starter\"] = df3[\"bal_savings_account_starter\"] - df1[\"bal_savings_account_starter\"]\n","  new_df[\"bal_change_current_starter\"] = df3[\"bal_current_account_starter\"] - df1[\"bal_current_account_starter\"]\n","  new_df[\"visits_change\"] = df3[\"visits_distinct_so\"] - df1[\"visits_distinct_so\"]\n","  #SOME STATISTICS EXPECTED TO BE IMPORTANT STATICALLY:\n","  new_df[\"customer_since_all\"] = df3[\"customer_since_all\"]\n","  new_df[\"customer_gender\"] = df3[\"customer_gender\"]\n","  new_df[\"customer_birth_date\"] = df3[\"customer_birth_date\"]\n","  new_df[\"customer_self_employed\"] = df3[\"customer_self_employed\"] \n","  new_df[\"has_reported_education\"] = df3[\"has_reported_education\"] \n","  new_df[\"brussels_postal_code\"] = df3[\"brussels_postal_code\"] \n","  new_df[\"flanders_postal_code\"] = df3[\"flanders_postal_code\"] \n","  new_df[\"wallonia_postal_code\"] = df3[\"wallonia_postal_code\"]\n","  new_df[\"other_postal_code\"] = df3[\"other_postal_code\"] \n","  new_df[\"target\"] = df3[\"target\"]\n","\n","\n","  cols_to_normalize = ['bal_change_insurance_21',\"bal_change_insurance_23\",\"cap_life_ins_fixed\",\"cap_life_ins_decreasing\",\"fire_premium_change\",\n","                      \"bal_change_personal_loan\",\"bal_change_mortgage_loan\",\"bal_change_current_account\",\"bal_change_pension_savings\",\"bal_change_savings\",\"bal_change_savings_starter\",\n","                      \"bal_change_current_starter\",\"visits_change\",\"customer_since_all\",\"customer_birth_date\"]\n","\n","  #Normalize the data\n","  if(transform == \"normal\"):\n","    for x in cols_to_normalize:\n","      new_df[x] = normalize_minmax(new_df[x])\n","\n","  elif transform == \"robust\"):\n","    trans = RobustScaler()\n","    new_df[cols_to_normalize] = trans.fit_transform(new_df[cols_to_normalize])\n","\n","  elif(transform == \"power\"): #For yeo power transform\n","    from sklearn.preprocessing import power_transform\n","    new_df[cols_to_normalize] = power_transform(new_df[cols_to_normalize],method='yeo-johnson') #Can handle full dataframes\n","\n","\n","return new_df\n"],"metadata":{"id":"fbxNpAP9tO6X","executionInfo":{"status":"ok","timestamp":1652164987724,"user_tz":-120,"elapsed":4,"user":{"displayName":"tijl coenen","userId":"13526193257448919759"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df_manual = get_manual_features_df(data=\"train\",transform=\"robust\",nan_handler=\"drop\",filename=None)\n","test_df_manual = get_manual_features_df(data=\"test\",transform=\"robust\",nan_handler=\"drop\",filename=None)"],"metadata":{"id":"hso6Hw03xSUR"},"execution_count":null,"outputs":[]}]}