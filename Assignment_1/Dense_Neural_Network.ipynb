{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Data_Augmentation_and_Model_Training.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN5hhuwY3N2neZ34HdLZFGY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"t0tCTpNyyDPO"},"outputs":[],"source":["\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","import pickle\n","import datetime as dt\n","\n","#Useful packages for building deep neural networks. \n","import tensorflow as tf \n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv1D,Flatten,Dense,Dropout, Reshape,MaxPooling2D,Conv2D\n","from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n","\n","#Additional library which we will use for preprocessing our image data before training our model and to provide some specific evaluation metrics.\n","import sklearn\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import classification_report"]},{"cell_type":"code","source":["#---------------- PREPARE DATAFRAME FOR MODEL TRAINING ----------------------------\n","X_train, y_train = np.array(train_df.loc[:, train_df.columns != 'target']),np.array(train_df.loc[:,'target'])\n","print('training features shape:{}'.format(X_train.shape))\n","print('training target shape:{}\\n'.format(y_train.shape))\n","\n","\n","#Apply SMOTE oversampling and random undersampling to handle the class imbalance\n","from imblearn.over_sampling import SMOTE\n","from imblearn.under_sampling import RandomUnderSampler\n","# oversample = SMOTE(sampling_strategy=0.1) #Generate new examples\n","# undersample = RandomUnderSampler(sampling_strategy=0.15) #undersample until we get a more equal distribution\n","# X_train,y_train = oversample.fit_resample(X_train, y_train)\n","# X_train,y_train = undersample.fit_resample(X_train, y_train)\n","\n","print(X_train.shape,y_train.shape)\n","\n","\n","# #Since we have a class imbalance let's create a dictionary with class weights to balance this. This step helps the model give equal attention to less frequent training examples, be making mistakes\n","#on these examples more costly.\n","classes = np.unique(y_train,return_counts=True)[0]\n","class_weights_arr = sklearn.utils.class_weight.compute_class_weight(class_weight = 'balanced', classes = classes, y = y_train)\n","\n","class_weights_dict = {} #input to model.fit requires dictionary\n","for i in classes:\n","    class_weights_dict[i] = class_weights_arr[i]\n","print(\"target attribute weights to handle class imbalance:{}\".format(class_weights_dict))"],"metadata":{"id":"Q7i8CM1eyQ5V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","#----------------- TRAIN & EVALUATE MODEL ------------------\n","#Set parameters\n","lr = 0.0001\n","momentum = 0.9\n","batch_size = 32\n","\n","# from keras import Sequential\n","model = Sequential()\n","# model.add(layers.Dense(256, activation=\"relu\", input_dim=X_train.shape[1]))\n","model.add(Dense(128,activation=\"relu\"))\n","model.add(Dropout(0.3))\n","model.add(Dense(64,activation=\"relu\"))\n","model.add(Dense(1,activation=\"sigmoid\"))\n","\n","model.compile(loss='binary_crossentropy', #integer encoded target labels, so sparse \n","              optimizer='adam',\n","              metrics=['accuracy',tf.keras.metrics.AUC()])\n","\n","model.fit(X_train,\n","          y_train,\n","          validation_split=0.1,\n","          batch_size = batch_size, \n","          class_weight = class_weights_dict,\n","          epochs=10\n","          )"],"metadata":{"id":"zsm2DmDSUSpF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#----------- DENSE NEURAL NETWORK -----------"],"metadata":{"id":"GzYAtP2wU0GG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Set parameters\n","lr = 0.0001\n","momentum = 0.9\n","batch_size = 64\n","# class_weights_dict_alt = {0:0.25,1:32} #Manuel weights to see if we can increase predictive performance.\n","\n","# from keras import Sequential\n","model = Sequential()\n","model.add(layers.Dense(256, activation=\"relu\", input_dim=X_train.shape[1]))\n","model.add(Dropout(0.3))\n","model.add(Dense(128,activation=\"relu\"))\n","model.add(Dropout(0.3))\n","model.add(Dense(32,activation=\"relu\"))\n","model.add(Dense(1,activation=\"sigmoid\"))\n","\n","model.compile(loss='binary_crossentropy', #integer encoded target labels, so sparse \n","              optimizer='adam',\n","              metrics=['accuracy',tf.keras.metrics.AUC()])\n","\n","model.fit(X_train,\n","          y_train,\n","          validation_split=0.1,\n","          batch_size = batch_size, \n","          class_weight = class_weights_dict,\n","          epochs=5\n","          )\n","\n","# predictions_proba = model.predict(X_test)\n","# predictions = []\n","# for x in predictions_proba:\n","#   if x>0.5:\n","#     predictions.append(1)\n","#   else:\n","#     predictions.append(0)\n","\n","# #AUC Curve is desirable here to evaluate the effect of different cut-off values for the predictions\n","\n","# print('Overall classification report:') \n","# print(classification_report(y_test,predictions))\n","\n","# print('\\nConfusion matrix:')\n","# sklearn.metrics.ConfusionMatrixDisplay.from_predictions(y_test, predictions, normalize = 'true') \n","# plt.show() "],"metadata":{"id":"etV4wwDTU2b4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#------------ HYPERPARAMETER TUNING -------------\n"],"metadata":{"id":"v7Sa52ooVLVV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#HYPERPARBAND ALGORITHM\n","# SOURCE: https://towardsdatascience.com/hyperparameter-tuning-with-kerastuner-and-tensorflow-c4a4d690b31a\n","# !pip install keras-tuner #When running for the first time again\n","import keras_tuner as kt\n","import tensorflow_addons as tfa\n","\n","#get base input shape\n","print(X_train.shape)\n","in_shp = list(X_train.shape[1:]) #47 dimensions\n","print(in_shp)\n","#input_nn = tensorflow.keras.Input(shape = (1,400)) #2D input for Convolutional layers\n","epochs = 20\n","\n","def build_model(hp):\n","    \"\"\"\n","    Builds model and sets up hyperparameter space to search. (For small dense network)\n","    \n","    Parameters\n","    ----------\n","    hp : HyperParameter object\n","        Configures hyperparameters to tune.\n","        \n","    Returns\n","    -------\n","    model : keras model\n","        Compiled model with hyperparameters to tune.\n","    \"\"\"\n","    # Initialize sequential API and start building model.\n","    model = tf.keras.Sequential()\n","    model.add(tf.keras.Input(shape=in_shp))\n","    # Tune the number of hidden layers and units in each.\n","    # Number of hidden layers: 1 - 5\n","    # Number of Units: 32 - 256 with stepsize of 32\n","    for i in range(1, hp.Int(\"num_layers\", 2, 6)):\n","        model.add(\n","            tf.keras.layers.Dense(\n","                units=hp.Int(\"units_\" + str(i), min_value=32, max_value=256, step=32),\n","                activation=\"relu\")\n","            )\n","        \n","        # Tune dropout layer with values from 0 - 0.3 with stepsize of 0.1.\n","        model.add(tf.keras.layers.Dropout(hp.Float(\"dropout_\" + str(i), 0, 0.3, step=0.1)))\n","    \n","    # Add output layer.\n","    model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n","    \n","    # Tune learning rate for Adam optimizer with values from 0.01, 0.001, or 0.0001\n","    hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n","    \n","    # Define optimizer, loss, and metrics\n","    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n","                  loss=tf.keras.losses.BinaryCrossentropy(),\n","                  metrics=[\"accuracy\",tfa.metrics.F1Score()])\n","    \n","    return model\n","\n","# Instantiate the tuner. HyperBand algorithm used for its effectiveness\n","\n","#Factor and max_epochs determine how many random models our search starts off to consider (log3(20+1)) = 3. So every iteration\n","#Considers 3 models in this case. It recognizes the base search space by permutations of the hp() objects defined in build_model\n","tuner = kt.Hyperband(build_model,\n","                     kt.Objective(\"val_f1\", direction=\"max\"), #documentation on flexible objective setting: https://keras.io/guides/keras_tuner/getting_started/\n","                     max_epochs=epochs,\n","                     factor=3, \n","                     hyperband_iterations=10)\n","                     #directory=\"kt_dir\", #directory to save progress to\n","                     #project_name=\"kt_hyperband\") #projectname = subdirectory under the main directory\n","\n","\n","# Display search space summary\n","tuner.search_space_summary()\n","\n","#tuner.search method takes similar input to mode.l.fit()\n","stop_early = tf.keras.callbacks.EarlyStopping(monitor=tfa.metrics.F1Score(), patience=5)\n","tuner.search(X_train, y_train, epochs=epochs, validation_split=0.2, callbacks=[stop_early], verbose=2)"],"metadata":{"id":"MnkhHwSvVJHi"},"execution_count":null,"outputs":[]}]}