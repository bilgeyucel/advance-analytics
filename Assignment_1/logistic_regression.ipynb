{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class BankModel:\n",
    "    def __init__(self ):\n",
    "        self.clf = None\n",
    "        self.features = None\n",
    "        self.test = None\n",
    "        \n",
    "    def fit(self, features, y_train, class_weight=None):\n",
    "        self.features = features\n",
    "        self.clf = LogisticRegression(class_weight=class_weight, random_state=1)\n",
    "        \n",
    "        # train the model\n",
    "        self.clf.fit(self.features, y_train)\n",
    "#         print(f\"The best parameters are {self.clf.best_params_} with a score of {self.clf.best_score_} on validation data\")\n",
    "    \n",
    "    def get_test_predict(self, text):\n",
    "        self.test = text\n",
    "        pred_val = self.predict(self.test)\n",
    "        return pred_val\n",
    "    \n",
    "    def get_test_predict_proba(self, text):\n",
    "        self.test = text\n",
    "        pred_prob = self.clf.predict_proba(self.test)\n",
    "        return pred_prob\n",
    "        \n",
    "    def get_metrics(self, y_test, pred_val):\n",
    "        print(\"Report for test data \\n\\n\", classification_report(y_test, pred_val))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.clf.predict(X)\n",
    "\n",
    "    def __call__(self, X):\n",
    "        return self.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"out/cleanedDataClassOccupation.csv\")\n",
    "before_one_month = pd.read_csv(\"out/cleaned_Train_2.csv\")\n",
    "before_two_months = pd.read_csv(\"out/cleaned_Train_1.csv\")\n",
    "extended_data = data\n",
    "# extended_data[\"bal_current_account_one_month\"] = before_one_month[\"bal_current_account\"]\n",
    "# extended_data[\"bal_current_account_two_month\"] = before_two_months[\"bal_current_account\"]\n",
    "# extended_data[\"bal_savings_account_one_month\"] = before_one_month[\"bal_savings_account\"]\n",
    "# extended_data[\"bal_savings_accountt_two_month\"] = before_two_months[\"bal_savings_account\"]\n",
    "# X = data.iloc[:, 1:38]  #independent columns\n",
    "# y = data[\"target\"]    #target column i.e price range\n",
    "# cleaned_data_wo_client_id = data_cleaned.loc[:, data_cleaned.columns != 'client_id']\n",
    "# prediction_data_probs = model.predict(prediction_data_wo_client_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63697, 39)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(extended_data.shape)\n",
    "# 1:extended_data.shape[1]\n",
    "features = extended_data.iloc[:, (extended_data.columns != 'target') & (extended_data.columns != 'client_id')] # all features \n",
    "X_train, X_test, y_train, y_test = train_test_split(features, extended_data[\"target\"] , test_size=0.15, stratify=data[\"target\"], random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Weight\n",
    "This is a crucial part of learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[ 0.515481   16.64883149]\n",
      "{0: 0.5154809962678041, 1: 16.648831488314883}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import compute_class_weight\n",
    "#Since we have a class imbalance let's create a dictionary with class weights to balance this. This step helps the model give equal attention to less frequent training examples, be making mistakes\n",
    "#on these examples more costly.\n",
    "classes = np.unique(y_train,return_counts=True)[0]\n",
    "class_weights_arr = compute_class_weight(class_weight = 'balanced', classes = classes, y = y_train)\n",
    "print(classes)\n",
    "print(class_weights_arr)\n",
    "\n",
    "class_weights_dict = {} #input to model.fit requires dictionary\n",
    "for i in classes:\n",
    "    class_weights_dict[i] = class_weights_arr[i]\n",
    "print(class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report for test data \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.67      0.79      9268\n",
      "           1       0.06      0.67      0.11       287\n",
      "\n",
      "    accuracy                           0.67      9555\n",
      "   macro avg       0.52      0.67      0.45      9555\n",
      "weighted avg       0.96      0.67      0.77      9555\n",
      "\n",
      "CPU times: user 1.82 s, sys: 59.1 ms, total: 1.88 s\n",
      "Wall time: 537 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = BankModel()\n",
    "model.fit(X_train, y_train, class_weight=class_weights_dict)\n",
    "model.get_metrics(y_test, model.get_test_predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST THE MODEL ON TEST DATA\n",
    "prediction_data = pd.read_csv(\"out/cleanedDataNoClassOccupation.csv\") # USE !pwd command to replace the path of `read_csv` if short ones dont work for you\n",
    "prediction_before_one_month = pd.read_csv(\"out/cleaned_Test_2.csv\")\n",
    "prediction_before_two_months = pd.read_csv(\"out/cleaned_Test_1.csv\")\n",
    "extended_prediction_data = prediction_data\n",
    "# extended_prediction_data[\"bal_current_account_one_month\"] = prediction_before_one_month[\"bal_current_account\"]\n",
    "# extended_prediction_data[\"bal_current_account_two_month\"] = prediction_before_two_months[\"bal_current_account\"]\n",
    "# extended_prediction_data[\"bal_savings_account_one_month\"] = prediction_before_one_month[\"bal_savings_account\"]\n",
    "# extended_prediction_data[\"bal_savings_account_two_month\"] = prediction_before_two_months[\"bal_savings_account\"]\n",
    "prediction_data_wo_client_id = extended_prediction_data.loc[:, extended_prediction_data.columns != 'client_id']\n",
    "prediction_data_probs = model.get_test_predict_proba(prediction_data_wo_client_id)\n",
    "probs = []\n",
    "for arr in prediction_data_probs:\n",
    "    probs.append(arr[1])\n",
    "prediction_data[\"target\"] = probs\n",
    "prediction_data[[\"client_id\",\"target\"]].to_csv('out/predictions.csv',\n",
    "                 sep=',', encoding='utf-8', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1913, 39)\n",
      "(61784, 39)\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.utils import resample\n",
    "\n",
    "# churn_data = data[data[\"target\"] == 1]\n",
    "# not_churn_data = data[data[\"target\"] == 0]\n",
    "\n",
    "# print(churn_data.shape)\n",
    "# print(not_churn_data.shape)\n",
    "# churn_upsample = resample(churn_data,\n",
    "#              replace=True,\n",
    "#              n_samples=len(not_churn_data),\n",
    "#              random_state=42)\n",
    "# upsampled_data = pd.concat([not_churn_data, churn_upsample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1913, 39)\n",
      "(61784, 39)\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.utils import resample\n",
    "\n",
    "# churn_data = data[data[\"target\"] == 1]\n",
    "# not_churn_data = data[data[\"target\"] == 0]\n",
    "\n",
    "# print(churn_data.shape)\n",
    "# print(not_churn_data.shape)\n",
    "# not_churn_downsample = resample(not_churn_data,\n",
    "#              replace=True,\n",
    "#              n_samples=len(churn_data),\n",
    "#              random_state=42)\n",
    "# downsampled_data = pd.concat([not_churn_downsample, churn_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report for test data \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.67      0.66      9268\n",
      "           1       0.66      0.65      0.66      9268\n",
      "\n",
      "    accuracy                           0.66     18536\n",
      "   macro avg       0.66      0.66      0.66     18536\n",
      "weighted avg       0.66      0.66      0.66     18536\n",
      "\n",
      "CPU times: user 3.31 s, sys: 102 ms, total: 3.41 s\n",
      "Wall time: 1.01 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# features = upsampled_data.iloc[:, (upsampled_data.columns != 'target') & (upsampled_data.columns != 'client_id')] # all features \n",
    "# X_train, X_test, y_train, y_test = train_test_split(features, upsampled_data[\"target\"] , test_size=0.15, stratify=upsampled_data[\"target\"], random_state=1)\n",
    "\n",
    "# model = BankModel()\n",
    "# model.fit(X_train, y_train, class_weight=class_weights_dict)\n",
    "# model.get_metrics(y_test, model.get_test_predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report for test data \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.69      0.68       287\n",
      "           1       0.68      0.65      0.67       287\n",
      "\n",
      "    accuracy                           0.67       574\n",
      "   macro avg       0.67      0.67      0.67       574\n",
      "weighted avg       0.67      0.67      0.67       574\n",
      "\n",
      "CPU times: user 217 ms, sys: 18.2 ms, total: 235 ms\n",
      "Wall time: 78.3 ms\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# features = downsampled_data.iloc[:, (downsampled_data.columns != 'target') & (downsampled_data.columns != 'client_id')] # all features \n",
    "# X_train, X_test, y_train, y_test = train_test_split(features, downsampled_data[\"target\"] , test_size=0.15, stratify=downsampled_data[\"target\"], random_state=1)\n",
    "\n",
    "# model = BankModel()\n",
    "# model.fit(X_train, y_train, class_weight=class_weights_dict)\n",
    "# model.get_metrics(y_test, model.get_test_predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cf1ee66cb0a5dbbe9b15b35d8b5ef163dfe6de55254de5ac3d70cf481ad0a057"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
